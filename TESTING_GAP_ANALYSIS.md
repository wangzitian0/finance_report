# Testing Gap Analysis & E2E Strategy

## é—®é¢˜ï¼šä¸ºä»€ä¹ˆæµ‹è¯•æ²¡æœ‰å‘ç° `primary_model` çš„å˜æ›´ï¼Ÿ

### æ ¹æœ¬åŸå› åˆ†æ

**å¤±è´¥çš„æµ‹è¯•**:
```python
# apps/backend/tests/test_main.py:284
def test_config_defaults(self):
    settings = Settings()
    assert settings.primary_model == "google/gemini-2.0-flash-exp:free"  # âŒ ç¡¬ç¼–ç æ—§å€¼
```

**é—®é¢˜**:
1. âœ… **æœ‰å•å…ƒæµ‹è¯•** - æµ‹è¯•å­˜åœ¨
2. âŒ **ç¡¬ç¼–ç æœŸæœ›å€¼** - æµ‹è¯•æ–­è¨€å†™æ­»äº†å…·ä½“çš„æ¨¡å‹åç§°
3. âŒ **ç¼ºå°‘å¥‘çº¦æµ‹è¯•** - æ²¡æœ‰æµ‹è¯•"é…ç½®å˜æ›´â†’æµ‹è¯•æ›´æ–°"çš„ä¸€è‡´æ€§
4. âŒ **æ²¡æœ‰ç«¯åˆ°ç«¯éªŒè¯** - æ— æ³•éªŒè¯å®é™…è¿è¡Œæ—¶é…ç½®æ˜¯å¦æ­£ç¡®

---

## å½“å‰æµ‹è¯•è¦†ç›–æƒ…å†µ

### 1. å•å…ƒæµ‹è¯• (Unit Tests) âœ… 96%

| å±‚çº§ | è¦†ç›–ç‡ | æµ‹è¯•æ•°é‡ | è¯´æ˜ |
|------|--------|----------|------|
| Models | 100% | ~50 | SQLAlchemy æ¨¡å‹ |
| Services | 95% | ~200 | ä¸šåŠ¡é€»è¾‘ |
| Routers | 98% | ~150 | API ç«¯ç‚¹ |
| Schemas | 100% | ~30 | Pydantic éªŒè¯ |

**ä¼˜ç‚¹**:
- âœ… é«˜è¦†ç›–ç‡ï¼ˆ96.26%ï¼‰
- âœ… å¿«é€Ÿæ‰§è¡Œï¼ˆ5-6 åˆ†é’Ÿï¼‰
- âœ… è‡ªåŠ¨åŒ– DB ç”Ÿå‘½å‘¨æœŸç®¡ç†

**ç¼ºç‚¹**:
- âŒ **æµ‹è¯•ä¸é…ç½®è€¦åˆ** - ç¡¬ç¼–ç æœŸæœ›å€¼
- âŒ **æ— æ³•å‘ç°é…ç½®æ¼‚ç§»** - ä»£ç æ”¹äº†ï¼Œæµ‹è¯•æ²¡æ”¹
- âŒ **ç¼ºå°‘è·¨å±‚éªŒè¯** - åªæµ‹å•ä¸ªç»„ä»¶

---

### 2. é›†æˆæµ‹è¯• (Integration Tests) âš ï¸ æœ‰é™

**ç°æœ‰é›†æˆæµ‹è¯•** (2ä¸ªæ–‡ä»¶):
```python
# apps/backend/tests/test_upload_integration.py
# - æµ‹è¯•ï¼šä¸Šä¼  â†’ å­˜å‚¨ â†’ æ•°æ®åº“ å®Œæ•´æµç¨‹
# - è¦†ç›–ï¼šStatements ä¸Šä¼  E2E æµç¨‹

# apps/backend/tests/test_accounting_integration.py  
# - æµ‹è¯•ï¼šåˆ›å»ºè´¦æˆ· â†’ è®°è´¦ â†’ ç»Ÿè®¡æŠ¥è¡¨
# - è¦†ç›–ï¼šJournal Entries å®Œæ•´ç”Ÿå‘½å‘¨æœŸ
```

**ç¼ºå¤±çš„é›†æˆæµ‹è¯•**:
- âŒ **AI æ¨¡å‹è°ƒç”¨é›†æˆ** - æ²¡æœ‰æµ‹è¯•å®é™…è°ƒç”¨ OpenRouter
- âŒ **é…ç½®åŠ è½½é›†æˆ** - æ²¡æœ‰æµ‹è¯• `.env` â†’ `config.py` â†’ è¿è¡Œæ—¶
- âŒ **å‰åç«¯é›†æˆ** - æ²¡æœ‰æµ‹è¯• API å¥‘çº¦ä¸€è‡´æ€§

---

### 3. ç«¯åˆ°ç«¯æµ‹è¯• (E2E Tests) âŒ ç¼ºå¤±

**Smoke Tests å­˜åœ¨ï¼Œä½†ä¸è¶³**:
```bash
# scripts/smoke_test.sh
# âœ… æµ‹è¯•é¡µé¢å¯è®¿é—®æ€§
# âœ… æµ‹è¯• /api/health
# âœ… æµ‹è¯• CORS
# âŒ ä¸æµ‹è¯•å®é™…åŠŸèƒ½æµç¨‹
# âŒ ä¸æµ‹è¯• AI æ¨¡å‹æ˜¯å¦çœŸçš„å¯ç”¨
```

**ç¼ºå¤±çš„ E2E åœºæ™¯**:
1. âŒ ç”¨æˆ·ä¸Šä¼  PDF â†’ AI è§£æ â†’ è¿”å›äº¤æ˜“æ•°æ®
2. âŒ ç”¨æˆ·é€‰æ‹©æ¨¡å‹ â†’ è°ƒç”¨æˆåŠŸ/å¤±è´¥å¤„ç†
3. âŒ localStorage æ¨¡å‹éªŒè¯ â†’ Fallback æµç¨‹
4. âŒ å¤šè´§å¸åœºæ™¯å®Œæ•´æµç¨‹

---

## æµ‹è¯•é‡‘å­—å¡”ç°çŠ¶ vs ç†æƒ³

### å½“å‰çŠ¶æ€ (ä¸å¹³è¡¡)

```
      E2E (0%)          â† âŒ ç¼ºå¤±
     /              \
   Integration (3%)    â† âš ï¸ ä¸è¶³
  /                  \
Unit Tests (96%)       â† âœ… è‰¯å¥½
```

### ç†æƒ³çŠ¶æ€

```
      E2E (5-10%)       â† å…³é”®è·¯å¾„
     /              \
   Integration (20%)   â† è·¨ç»„ä»¶äº¤äº’
  /                  \
Unit Tests (70-75%)    â† å¿«é€Ÿåé¦ˆ
```

---

## å…·ä½“æ”¹è¿›å»ºè®®

### çŸ­æœŸ (1-2 å‘¨)

#### 1. ä¿®å¤é…ç½®æµ‹è¯•çš„è„†å¼±æ€§

**é—®é¢˜**: `test_config_defaults` ç¡¬ç¼–ç æœŸæœ›å€¼

**æ–¹æ¡ˆ A: ç¯å¢ƒå˜é‡é©±åŠ¨** (æ¨è)
```python
# apps/backend/tests/test_main.py
def test_config_defaults(self):
    """Test Settings has reasonable defaults."""
    from src.config import Settings
    
    settings = Settings()
    # âœ… ä» .env.example è¯»å–æœŸæœ›å€¼ï¼Œæˆ–ä½¿ç”¨åˆç†çš„æ–­è¨€
    assert settings.primary_model.startswith("google/gemini")  # å®½æ¾æ–­è¨€
    assert "gemini" in settings.primary_model.lower()
    assert settings.s3_bucket == "statements"
```

**æ–¹æ¡ˆ B: å¥‘çº¦æµ‹è¯•**
```python
# apps/backend/tests/test_config_contract.py
import re

def test_config_primary_model_contract():
    """Ensure primary_model follows expected pattern."""
    from src.config import Settings
    
    settings = Settings()
    # âœ… æµ‹è¯•å¥‘çº¦ï¼Œè€Œéå…·ä½“å€¼
    assert re.match(r'^google/gemini-\d+\.\d+-.*$', settings.primary_model), \
        f"Invalid model format: {settings.primary_model}"
    
def test_config_sync_with_env_example():
    """Ensure config.py default matches .env.example."""
    import os
    from pathlib import Path
    from src.config import Settings
    
    settings = Settings()
    env_example = Path(".env.example").read_text()
    
    # ä» .env.example è§£æ PRIMARY_MODEL çš„é»˜è®¤å€¼
    match = re.search(r'^PRIMARY_MODEL=(.*)$', env_example, re.MULTILINE)
    if match:
        expected = match.group(1).strip()
        assert settings.primary_model == expected, \
            f"config.py default ({settings.primary_model}) != .env.example ({expected})"
```

#### 2. æ·»åŠ  AI æ¨¡å‹è°ƒç”¨é›†æˆæµ‹è¯•

**é—®é¢˜**: æ²¡æœ‰æµ‹è¯•å®é™…è°ƒç”¨ OpenRouter æ˜¯å¦æˆåŠŸ

**æ–¹æ¡ˆ**: Mock + Real Call æ··åˆ
```python
# apps/backend/tests/test_ai_models_integration.py
import pytest
from unittest.mock import patch, MagicMock

@pytest.mark.integration
async def test_get_models_from_openrouter():
    """Test fetching models from OpenRouter (real call)."""
    from src.services.openrouter_models import get_models
    
    models = await get_models()
    assert len(models) > 0
    assert any("gemini" in m["id"].lower() for m in models)

@pytest.mark.integration
async def test_primary_model_exists_in_catalog():
    """Test that config.PRIMARY_MODEL exists in OpenRouter catalog."""
    from src.config import settings
    from src.services.openrouter_models import get_models
    
    models = await get_models()
    model_ids = [m["id"] for m in models]
    
    assert settings.primary_model in model_ids, \
        f"PRIMARY_MODEL '{settings.primary_model}' not found in OpenRouter catalog"

@pytest.mark.integration  
async def test_invalid_model_raises_400(client, test_user):
    """Test uploading with invalid model returns 400."""
    response = await client.post(
        "/api/statements/upload",
        headers={"X-User-Id": str(test_user.id)},
        data={"model": "invalid-model-id"},
        files={"file": ("test.pdf", b"dummy", "application/pdf")}
    )
    assert response.status_code == 400
    assert "Invalid model" in response.json()["detail"]
```

#### 3. æ·»åŠ å‰ç«¯æ¨¡å‹éªŒè¯é›†æˆæµ‹è¯•

**é—®é¢˜**: æ²¡æœ‰æµ‹è¯• localStorage éªŒè¯é€»è¾‘

**æ–¹æ¡ˆ**: Playwright E2E æµ‹è¯•
```python
# apps/frontend/tests/e2e/test_model_validation.py
import pytest
from playwright.async_api import async_playwright

@pytest.mark.e2e
async def test_stale_model_id_auto_cleanup():
    """Test that stale localStorage model IDs are auto-cleared."""
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()
        
        # 1. Inject stale model ID
        await page.goto("http://localhost:3000/statements")
        await page.evaluate(
            'localStorage.setItem("statement_model_v1", "google/gemini-2.0-flash-thinking")'
        )
        
        # 2. Reload page
        await page.reload()
        await page.wait_for_selector('[data-testid="model-selector"]')
        
        # 3. Verify auto-cleanup
        stored = await page.evaluate('localStorage.getItem("statement_model_v1")')
        assert stored is None or stored != "google/gemini-2.0-flash-thinking", \
            "Stale model ID was not cleared"
        
        # 4. Verify fallback to default
        selected = await page.locator('[data-testid="model-selector"]').input_value()
        assert "gemini-3-flash-preview" in selected
        
        await browser.close()
```

---

### ä¸­æœŸ (1-2 æœˆ)

#### 4. å»ºç«‹ E2E æµ‹è¯•å¥—ä»¶

**ç›®æ ‡**: è¦†ç›– 5 ä¸ªå…³é”®ç”¨æˆ·æ—…ç¨‹

**å·¥å…·é€‰æ‹©**: Playwright (å·²ç»åœ¨ç”¨ pytest-playwright)

**å…³é”®åœºæ™¯**:

1. **Statement Upload E2E**
```python
# tests/e2e/test_statement_upload_e2e.py
@pytest.mark.e2e
async def test_upload_pdf_statement_full_flow():
    """
    E2E: ç”¨æˆ·ä¸Šä¼  PDF â†’ AI è§£æ â†’ æŸ¥çœ‹äº¤æ˜“ â†’ å®¡æ‰¹
    """
    # 1. Login
    # 2. Navigate to /statements
    # 3. Upload PDF file
    # 4. Wait for parsing (check progress)
    # 5. View extracted transactions
    # 6. Approve/Reject
    # 7. Verify in journal
```

2. **Model Selection E2E**
```python
@pytest.mark.e2e
async def test_model_selection_and_upload():
    """
    E2E: ç”¨æˆ·é€‰æ‹©ä¸åŒæ¨¡å‹ â†’ ä¸Šä¼  â†’ éªŒè¯è°ƒç”¨æ­£ç¡®æ¨¡å‹
    """
    # 1. Navigate to /statements
    # 2. Select Gemini 3 from dropdown
    # 3. Upload file
    # 4. Verify backend logs show correct model
```

3. **Reconciliation E2E**
```python
@pytest.mark.e2e
async def test_reconciliation_full_flow():
    """
    E2E: ä¸Šä¼ è´¦å• â†’ è‡ªåŠ¨å¯¹è´¦ â†’ å®¡æ ¸é˜Ÿåˆ— â†’ æ‰¹å‡†
    """
    # 1. Upload bank statement
    # 2. Create manual journal entries
    # 3. Run reconciliation
    # 4. Check pending review queue
    # 5. Approve matches
```

4. **Multi-Currency E2E**
```python
@pytest.mark.e2e
async def test_multi_currency_reporting():
    """
    E2E: å¤šè´§å¸è´¦æˆ· â†’ äº¤æ˜“ â†’ æŠ¥è¡¨ç”Ÿæˆ
    """
    # 1. Create USD account
    # 2. Create SGD account
    # 3. Add transactions
    # 4. Generate balance sheet (SGD)
    # 5. Verify FX conversion
```

5. **Error Handling E2E**
```python
@pytest.mark.e2e
async def test_openrouter_failure_handling():
    """
    E2E: OpenRouter å¤±è´¥ â†’ æ˜¾ç¤ºé”™è¯¯ â†’ Fallback æ¨¡å‹
    """
    # 1. Mock OpenRouter 503
    # 2. Upload statement
    # 3. Verify user sees error message
    # 4. Verify fallback model attempted
```

---

#### 5. æ·»åŠ  Smoke Test åˆ° CI

**é—®é¢˜**: `smoke_test.sh` å­˜åœ¨ä½†æœªåœ¨ CI ä¸­è¿è¡Œ

**æ–¹æ¡ˆ**: åœ¨ PR ç¯å¢ƒéƒ¨ç½²åè¿è¡Œ smoke tests

```yaml
# .github/workflows/pr-test.yml
jobs:
  deploy-pr-env:
    # ... existing deploy steps ...
    
  smoke-test:
    needs: deploy-pr-env
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Run Smoke Tests
        run: |
          bash scripts/smoke_test.sh \
            "https://report-pr-${{ needs.deploy-pr-env.outputs.pr_number }}.zitian.party" \
            "staging"
      
      - name: Health Check Summary
        if: always()
        run: |
          echo "::notice::Smoke tests completed for PR #${{ needs.deploy-pr-env.outputs.pr_number }}"
```

---

#### 6. æµ‹è¯•æ•°æ®ç®¡ç†ç­–ç•¥

**é—®é¢˜**: æ¯ä¸ªæµ‹è¯•éƒ½æ‰‹åŠ¨åˆ›å»ºæ•°æ®ï¼Œé‡å¤ä»£ç å¤š

**æ–¹æ¡ˆ**: Fixture Factory Pattern

```python
# apps/backend/tests/factories.py
from decimal import Decimal
from datetime import date
import factory

class AccountFactory(factory.Factory):
    class Meta:
        model = Account
    
    name = factory.Sequence(lambda n: f"Account {n}")
    type = AccountType.ASSET
    currency = "SGD"
    is_active = True

class JournalEntryFactory(factory.Factory):
    class Meta:
        model = JournalEntry
    
    entry_date = factory.LazyFunction(lambda: date.today())
    memo = "Test Transaction"
    status = JournalEntryStatus.POSTED
    
    @factory.post_generation
    def lines(self, create, extracted, **kwargs):
        if not create:
            return
        # Auto-generate balanced lines
        debit_account = AccountFactory(type=AccountType.ASSET)
        credit_account = AccountFactory(type=AccountType.INCOME)
        JournalLineFactory(entry=self, account=debit_account, direction=Direction.DEBIT, amount=100)
        JournalLineFactory(entry=self, account=credit_account, direction=Direction.CREDIT, amount=100)

# Usage in tests:
async def test_something(db):
    entry = JournalEntryFactory.create_async(user_id=user.id)
    await db.commit()
    # ...
```

---

### é•¿æœŸ (3-6 æœˆ)

#### 7. Visual Regression Testing

**é—®é¢˜**: UI å˜æ›´éš¾ä»¥å‘ç°

**æ–¹æ¡ˆ**: Percy.io / Playwright Screenshots

```python
# tests/e2e/test_visual_regression.py
@pytest.mark.visual
async def test_dashboard_visual():
    """Screenshot comparison for dashboard."""
    page = await browser.new_page()
    await page.goto("http://localhost:3000/dashboard")
    await page.wait_for_selector('[data-testid="dashboard-loaded"]')
    await page.screenshot(path="screenshots/dashboard.png")
    # Percy compares with baseline
```

---

#### 8. Performance Testing

**é—®é¢˜**: æ— æ€§èƒ½åŸºå‡†

**æ–¹æ¡ˆ**: Locust / k6 è´Ÿè½½æµ‹è¯•

```python
# tests/performance/locustfile.py
from locust import HttpUser, task, between

class FinanceReportUser(HttpUser):
    wait_time = between(1, 3)
    
    @task
    def view_dashboard(self):
        self.client.get("/dashboard")
    
    @task(3)  # æƒé‡æ›´é«˜
    def upload_statement(self):
        with open("fixtures/sample.pdf", "rb") as f:
            self.client.post("/api/statements/upload", files={"file": f})
```

---

## CI/CD æµ‹è¯•ç­–ç•¥

### æµ‹è¯•åˆ†å±‚æ‰§è¡Œ

| é˜¶æ®µ | æµ‹è¯•ç±»å‹ | è§¦å‘æ—¶æœº | é¢„æœŸæ—¶é•¿ |
|------|----------|----------|----------|
| **Pre-commit** | Lint, Format | Git hook | < 5s |
| **PR Open** | Unit Tests (96%) | Every push | 5-6 min |
| **PR Ready** | Integration Tests | After unit pass | 2-3 min |
| **PR Deploy** | Smoke Tests | After PR env deploy | 1 min |
| **Pre-merge** | E2E Tests (Critical) | Before merge | 5-10 min |
| **Post-merge** | Full E2E Suite | After merge to main | 15-20 min |
| **Nightly** | Performance Tests | Scheduled | 30 min |

---

## æµ‹è¯•å‘½ä»¤è§„èŒƒ

```bash
# æœ¬åœ°å¼€å‘
moon run :test              # å•å…ƒæµ‹è¯• (å¿«é€Ÿ)
moon run :test-integration  # é›†æˆæµ‹è¯• (æ–°å¢)
moon run :test-e2e          # E2E æµ‹è¯• (æ–°å¢)

# CI ä¸“ç”¨
moon run :test                 # å…¨éƒ¨æµ‹è¯•
moon run :test -- --fast            # å…³é”®è·¯å¾„æµ‹è¯• (å¿«é€ŸéªŒè¯)
moon run :smoke                    # Smoke æµ‹è¯• (å·²å­˜åœ¨)

# è¦†ç›–ç‡
moon run :test          # ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
```

---

## æµ‹è¯•å¯è§‚æµ‹æ€§

### 1. æµ‹è¯•æŠ¥å‘Šé¢æ¿

**å·¥å…·**: Pytest HTML Report + Allure

```bash
# ç”Ÿæˆ HTML æŠ¥å‘Š
pytest --html=report.html --self-contained-html

# ä¸Šä¼ åˆ° GitHub Actions Artifacts
- uses: actions/upload-artifact@v4
  with:
    name: test-report
    path: report.html
```

### 2. æµ‹è¯•å¤±è´¥é€šçŸ¥

**æ–¹æ¡ˆ**: GitHub Actions â†’ Slack / Email

```yaml
- name: Notify on Test Failure
  if: failure()
  uses: 8398a7/action-slack@v3
  with:
    status: ${{ job.status }}
    text: "Tests failed on PR #${{ github.event.pull_request.number }}"
```

### 3. è¶‹åŠ¿åˆ†æ

**å·¥å…·**: Codecov / Coveralls (å·²é›†æˆ)

**æ‰©å±•**: æµ‹è¯•æ‰§è¡Œæ—¶é—´è¶‹åŠ¿ã€å¤±è´¥ç‡è¶‹åŠ¿

---

## ä¼˜å…ˆçº§çŸ©é˜µ

| æ”¹è¿›é¡¹ | å½±å“åŠ› | å®æ–½æˆæœ¬ | ä¼˜å…ˆçº§ |
|--------|--------|----------|--------|
| ä¿®å¤é…ç½®æµ‹è¯•è„†å¼±æ€§ | ğŸ”¥ High | ğŸ’° Low | **P0** (ç«‹å³) |
| AI æ¨¡å‹è°ƒç”¨é›†æˆæµ‹è¯• | ğŸ”¥ High | ğŸ’° Medium | **P0** (æœ¬å‘¨) |
| Smoke Tests é›†æˆåˆ° CI | ğŸ”¥ High | ğŸ’° Low | **P0** (æœ¬å‘¨) |
| å‰ç«¯æ¨¡å‹éªŒè¯ E2E | ğŸ”¥ High | ğŸ’° High | **P1** (2å‘¨å†…) |
| å®Œæ•´ E2E æµ‹è¯•å¥—ä»¶ | ğŸ”¥ High | ğŸ’°ğŸ’° High | **P1** (1æœˆå†…) |
| æµ‹è¯•æ•°æ®å·¥å‚ | ğŸ”µ Medium | ğŸ’° Medium | **P2** (2æœˆå†…) |
| Visual Regression | ğŸ”µ Medium | ğŸ’°ğŸ’° High | **P3** (3æœˆå†…) |
| Performance Testing | ğŸ”µ Low | ğŸ’°ğŸ’° High | **P3** (æŒ‰éœ€) |

---

## æˆåŠŸæŒ‡æ ‡

**çŸ­æœŸç›®æ ‡ (1ä¸ªæœˆ)**:
- [ ] é…ç½®æµ‹è¯•ä¸å†ç¡¬ç¼–ç æœŸæœ›å€¼
- [ ] AI æ¨¡å‹é›†æˆæµ‹è¯•è¦†ç›–ç‡ > 80%
- [ ] Smoke Tests åœ¨ CI ä¸­è¿è¡Œ
- [ ] è‡³å°‘ 3 ä¸ªå…³é”® E2E åœºæ™¯è¦†ç›–

**ä¸­æœŸç›®æ ‡ (3ä¸ªæœˆ)**:
- [ ] E2E æµ‹è¯•è¦†ç›– 5 ä¸ªæ ¸å¿ƒç”¨æˆ·æ—…ç¨‹
- [ ] æµ‹è¯•æ€»æ‰§è¡Œæ—¶é—´ < 15 åˆ†é’Ÿ
- [ ] æµ‹è¯•å¤±è´¥æ—¶æœ‰æ˜ç¡®çš„é”™è¯¯ä¿¡æ¯
- [ ] PR ç¯å¢ƒè‡ªåŠ¨è¿è¡Œ smoke tests

**é•¿æœŸç›®æ ‡ (6ä¸ªæœˆ)**:
- [ ] æµ‹è¯•é‡‘å­—å¡”å¹³è¡¡ï¼ˆ70% Unit, 20% Integration, 10% E2Eï¼‰
- [ ] æ€§èƒ½åŸºå‡†å»ºç«‹ï¼ˆP95 < 500msï¼‰
- [ ] Visual Regression è¦†ç›–å…³é”®é¡µé¢
- [ ] æµ‹è¯•å¯è§‚æµ‹æ€§é¢æ¿ä¸Šçº¿

---

## æ€»ç»“

### ä¸ºä»€ä¹ˆæµ‹è¯•æ²¡å‘ç°ï¼Ÿ

1. **æµ‹è¯•å­˜åœ¨** âœ… ä½†æ˜¯ **æ–­è¨€ç¡¬ç¼–ç ** âŒ
2. **å•å…ƒæµ‹è¯•å……è¶³** âœ… ä½†æ˜¯ **é›†æˆæµ‹è¯•ä¸è¶³** âš ï¸
3. **Smoke Tests å­˜åœ¨** âœ… ä½†æ˜¯ **æœªåœ¨ CI è¿è¡Œ** âŒ
4. **ç¼ºå°‘ç«¯åˆ°ç«¯éªŒè¯** âŒ æ— æ³•å‘ç°å®é™…è¿è¡Œæ—¶é—®é¢˜

### æœ€å…³é”®çš„ 3 ä¸ªæ”¹è¿›

1. **é…ç½®å¥‘çº¦æµ‹è¯•** - é˜²æ­¢ç±»ä¼¼é—®é¢˜å†æ¬¡å‘ç”Ÿ
2. **AI æ¨¡å‹é›†æˆæµ‹è¯•** - éªŒè¯å®é™… API è°ƒç”¨
3. **Smoke Tests in CI** - æ¯æ¬¡éƒ¨ç½²åéªŒè¯åŸºæœ¬åŠŸèƒ½

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨

- [ ] åˆ›å»º Issue #151: "Add config contract tests"
- [ ] åˆ›å»º Issue #152: "Add AI model integration tests"
- [ ] åˆ›å»º Issue #153: "Integrate smoke tests into PR CI"
- [ ] åˆ›å»º Epic: "E2E Testing Infrastructure (Phase 1)"
